{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of GAN's(Generative Adversarial Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 200\n",
    "imageSize = 64\n",
    "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "dataset = dset.MNIST(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/15][0/300] Loss_D: 1.6604 Loss_G: 3.9173\n",
      "[0/15][1/300] Loss_D: 1.4699 Loss_G: 5.9038\n",
      "[0/15][2/300] Loss_D: 0.3057 Loss_G: 7.0678\n",
      "[0/15][3/300] Loss_D: 0.1837 Loss_G: 6.6059\n",
      "[0/15][4/300] Loss_D: 0.2736 Loss_G: 7.1649\n",
      "[0/15][5/300] Loss_D: 0.2749 Loss_G: 7.9735\n",
      "[0/15][6/300] Loss_D: 0.2960 Loss_G: 8.3451\n",
      "[0/15][7/300] Loss_D: 0.2007 Loss_G: 7.2071\n",
      "[0/15][8/300] Loss_D: 0.2394 Loss_G: 9.1321\n",
      "[0/15][9/300] Loss_D: 0.0938 Loss_G: 8.3436\n",
      "[0/15][10/300] Loss_D: 0.1781 Loss_G: 9.2702\n",
      "[0/15][11/300] Loss_D: 0.0989 Loss_G: 8.2909\n",
      "[0/15][12/300] Loss_D: 0.1325 Loss_G: 9.8159\n",
      "[0/15][13/300] Loss_D: 0.1221 Loss_G: 7.2382\n",
      "[0/15][14/300] Loss_D: 0.4065 Loss_G: 17.4491\n",
      "[0/15][15/300] Loss_D: 0.7042 Loss_G: 10.9167\n",
      "[0/15][16/300] Loss_D: 0.1665 Loss_G: 9.0697\n",
      "[0/15][17/300] Loss_D: 0.7708 Loss_G: 21.0870\n",
      "[0/15][18/300] Loss_D: 0.1687 Loss_G: 20.7460\n",
      "[0/15][19/300] Loss_D: 0.1517 Loss_G: 11.2286\n",
      "[0/15][20/300] Loss_D: 0.3561 Loss_G: 15.1864\n",
      "[0/15][21/300] Loss_D: 0.0444 Loss_G: 14.8519\n",
      "[0/15][22/300] Loss_D: 0.0855 Loss_G: 9.2842\n",
      "[0/15][23/300] Loss_D: 0.1900 Loss_G: 11.6438\n",
      "[0/15][24/300] Loss_D: 0.0592 Loss_G: 9.5325\n",
      "[0/15][25/300] Loss_D: 0.1546 Loss_G: 11.7776\n",
      "[0/15][26/300] Loss_D: 0.0728 Loss_G: 9.2300\n",
      "[0/15][27/300] Loss_D: 0.1971 Loss_G: 14.8452\n",
      "[0/15][28/300] Loss_D: 0.0773 Loss_G: 12.9725\n",
      "[0/15][29/300] Loss_D: 0.0466 Loss_G: 7.0315\n",
      "[0/15][30/300] Loss_D: 0.7507 Loss_G: 25.7907\n",
      "[0/15][31/300] Loss_D: 1.2814 Loss_G: 24.4531\n",
      "[0/15][32/300] Loss_D: 0.0023 Loss_G: 14.8813\n",
      "[0/15][33/300] Loss_D: 0.8380 Loss_G: 24.3522\n",
      "[0/15][34/300] Loss_D: 0.1348 Loss_G: 25.8909\n",
      "[0/15][35/300] Loss_D: 0.2834 Loss_G: 20.5345\n",
      "[0/15][36/300] Loss_D: 0.0337 Loss_G: 7.7164\n",
      "[0/15][37/300] Loss_D: 2.2765 Loss_G: 27.6174\n",
      "[0/15][38/300] Loss_D: 9.1324 Loss_G: 27.2778\n",
      "[0/15][39/300] Loss_D: 0.0318 Loss_G: 25.9835\n",
      "[0/15][40/300] Loss_D: 0.0198 Loss_G: 19.2827\n",
      "[0/15][41/300] Loss_D: 0.0098 Loss_G: 5.4379\n",
      "[0/15][42/300] Loss_D: 5.3147 Loss_G: 26.5596\n",
      "[0/15][43/300] Loss_D: 3.9796 Loss_G: 24.2124\n",
      "[0/15][44/300] Loss_D: 0.1952 Loss_G: 17.5253\n",
      "[0/15][45/300] Loss_D: 0.0814 Loss_G: 5.6212\n",
      "[0/15][46/300] Loss_D: 4.5503 Loss_G: 20.4864\n",
      "[0/15][47/300] Loss_D: 0.6439 Loss_G: 23.5690\n",
      "[0/15][48/300] Loss_D: 1.3895 Loss_G: 17.7899\n",
      "[0/15][49/300] Loss_D: 0.2404 Loss_G: 9.2383\n",
      "[0/15][50/300] Loss_D: 0.3021 Loss_G: 8.3371\n",
      "[0/15][51/300] Loss_D: 0.4653 Loss_G: 14.6877\n",
      "[0/15][52/300] Loss_D: 0.1534 Loss_G: 12.7997\n",
      "[0/15][53/300] Loss_D: 0.2684 Loss_G: 7.0025\n",
      "[0/15][54/300] Loss_D: 2.2056 Loss_G: 24.2318\n",
      "[0/15][55/300] Loss_D: 1.9632 Loss_G: 24.8416\n",
      "[0/15][56/300] Loss_D: 0.0288 Loss_G: 21.1783\n",
      "[0/15][57/300] Loss_D: 0.0113 Loss_G: 10.1119\n",
      "[0/15][58/300] Loss_D: 2.2596 Loss_G: 23.5269\n",
      "[0/15][59/300] Loss_D: 0.0101 Loss_G: 26.0014\n",
      "[0/15][60/300] Loss_D: 0.1863 Loss_G: 24.6801\n",
      "[0/15][61/300] Loss_D: 0.2048 Loss_G: 19.3552\n",
      "[0/15][62/300] Loss_D: 0.0313 Loss_G: 11.0145\n",
      "[0/15][63/300] Loss_D: 0.1076 Loss_G: 6.2166\n",
      "[0/15][64/300] Loss_D: 1.0047 Loss_G: 19.4354\n",
      "[0/15][65/300] Loss_D: 1.0264 Loss_G: 17.2591\n",
      "[0/15][66/300] Loss_D: 0.2038 Loss_G: 8.3499\n",
      "[0/15][67/300] Loss_D: 1.1799 Loss_G: 18.4164\n",
      "[0/15][68/300] Loss_D: 1.4591 Loss_G: 11.9333\n",
      "[0/15][69/300] Loss_D: 0.0856 Loss_G: 5.0903\n",
      "[0/15][70/300] Loss_D: 0.6574 Loss_G: 12.9541\n",
      "[0/15][71/300] Loss_D: 0.3120 Loss_G: 11.8860\n",
      "[0/15][72/300] Loss_D: 0.1646 Loss_G: 7.2171\n",
      "[0/15][73/300] Loss_D: 0.0800 Loss_G: 4.5133\n",
      "[0/15][74/300] Loss_D: 0.2511 Loss_G: 8.2909\n",
      "[0/15][75/300] Loss_D: 0.2167 Loss_G: 6.2037\n",
      "[0/15][76/300] Loss_D: 0.1765 Loss_G: 4.5528\n",
      "[0/15][77/300] Loss_D: 0.2325 Loss_G: 6.3624\n",
      "[0/15][78/300] Loss_D: 0.2358 Loss_G: 3.6515\n",
      "[0/15][79/300] Loss_D: 0.3148 Loss_G: 8.1495\n",
      "[0/15][80/300] Loss_D: 0.4487 Loss_G: 1.9233\n",
      "[0/15][81/300] Loss_D: 1.0375 Loss_G: 14.6964\n",
      "[0/15][82/300] Loss_D: 2.0867 Loss_G: 3.7980\n",
      "[0/15][83/300] Loss_D: 0.2660 Loss_G: 3.0620\n",
      "[0/15][84/300] Loss_D: 0.4681 Loss_G: 8.3639\n",
      "[0/15][85/300] Loss_D: 0.1273 Loss_G: 8.5542\n",
      "[0/15][86/300] Loss_D: 0.3065 Loss_G: 4.8574\n",
      "[0/15][87/300] Loss_D: 0.1846 Loss_G: 4.2812\n",
      "[0/15][88/300] Loss_D: 0.2120 Loss_G: 6.0611\n",
      "[0/15][89/300] Loss_D: 0.1538 Loss_G: 5.3366\n",
      "[0/15][90/300] Loss_D: 0.1220 Loss_G: 4.2582\n",
      "[0/15][91/300] Loss_D: 0.1430 Loss_G: 4.6472\n",
      "[0/15][92/300] Loss_D: 0.1204 Loss_G: 4.9943\n",
      "[0/15][93/300] Loss_D: 0.1667 Loss_G: 4.2221\n",
      "[0/15][94/300] Loss_D: 0.1389 Loss_G: 5.1556\n",
      "[0/15][95/300] Loss_D: 0.1249 Loss_G: 4.7832\n",
      "[0/15][96/300] Loss_D: 0.1213 Loss_G: 4.6471\n",
      "[0/15][97/300] Loss_D: 0.1628 Loss_G: 6.1344\n",
      "[0/15][98/300] Loss_D: 0.1507 Loss_G: 4.7435\n",
      "[0/15][99/300] Loss_D: 0.1633 Loss_G: 5.4725\n",
      "[0/15][100/300] Loss_D: 0.1075 Loss_G: 5.2941\n",
      "[0/15][101/300] Loss_D: 0.1159 Loss_G: 4.8463\n",
      "[0/15][102/300] Loss_D: 0.0893 Loss_G: 5.3691\n",
      "[0/15][103/300] Loss_D: 0.1166 Loss_G: 4.9542\n",
      "[0/15][104/300] Loss_D: 0.0900 Loss_G: 5.1179\n",
      "[0/15][105/300] Loss_D: 0.0717 Loss_G: 5.8201\n",
      "[0/15][106/300] Loss_D: 0.0784 Loss_G: 5.4159\n",
      "[0/15][107/300] Loss_D: 0.0877 Loss_G: 5.4770\n",
      "[0/15][108/300] Loss_D: 0.0955 Loss_G: 6.1810\n",
      "[0/15][109/300] Loss_D: 0.1079 Loss_G: 5.5558\n",
      "[0/15][110/300] Loss_D: 0.1573 Loss_G: 7.2206\n",
      "[0/15][111/300] Loss_D: 0.1124 Loss_G: 5.7714\n",
      "[0/15][112/300] Loss_D: 0.1750 Loss_G: 6.8509\n",
      "[0/15][113/300] Loss_D: 0.1311 Loss_G: 6.1615\n",
      "[0/15][114/300] Loss_D: 0.2113 Loss_G: 7.4862\n",
      "[0/15][115/300] Loss_D: 0.1641 Loss_G: 5.2481\n",
      "[0/15][116/300] Loss_D: 0.1683 Loss_G: 7.5210\n",
      "[0/15][117/300] Loss_D: 0.1210 Loss_G: 6.2851\n",
      "[0/15][118/300] Loss_D: 0.1058 Loss_G: 5.2843\n",
      "[0/15][119/300] Loss_D: 0.1719 Loss_G: 7.5412\n",
      "[0/15][120/300] Loss_D: 0.1492 Loss_G: 5.2988\n",
      "[0/15][121/300] Loss_D: 0.1063 Loss_G: 5.7865\n",
      "[0/15][122/300] Loss_D: 0.1259 Loss_G: 6.2751\n",
      "[0/15][123/300] Loss_D: 0.1449 Loss_G: 4.3717\n",
      "[0/15][124/300] Loss_D: 0.1994 Loss_G: 9.0380\n",
      "[0/15][125/300] Loss_D: 0.2693 Loss_G: 4.4105\n",
      "[0/15][126/300] Loss_D: 0.1396 Loss_G: 5.4396\n",
      "[0/15][127/300] Loss_D: 0.0631 Loss_G: 6.3155\n",
      "[0/15][128/300] Loss_D: 0.0794 Loss_G: 5.2898\n",
      "[0/15][129/300] Loss_D: 0.0726 Loss_G: 4.6219\n",
      "[0/15][130/300] Loss_D: 0.0944 Loss_G: 5.4493\n",
      "[0/15][131/300] Loss_D: 0.1533 Loss_G: 3.6237\n",
      "[0/15][132/300] Loss_D: 0.1710 Loss_G: 6.8484\n",
      "[0/15][133/300] Loss_D: 0.2074 Loss_G: 3.6795\n",
      "[0/15][134/300] Loss_D: 0.1674 Loss_G: 5.9056\n",
      "[0/15][135/300] Loss_D: 0.0749 Loss_G: 5.6154\n",
      "[0/15][136/300] Loss_D: 0.1265 Loss_G: 3.1721\n",
      "[0/15][137/300] Loss_D: 0.2697 Loss_G: 8.5961\n",
      "[0/15][138/300] Loss_D: 0.7941 Loss_G: 0.0871\n",
      "[0/15][139/300] Loss_D: 3.2314 Loss_G: 17.0876\n",
      "[0/15][140/300] Loss_D: 7.7294 Loss_G: 4.8724\n",
      "[0/15][141/300] Loss_D: 0.2516 Loss_G: 3.1826\n",
      "[0/15][142/300] Loss_D: 0.5973 Loss_G: 6.0122\n",
      "[0/15][143/300] Loss_D: 1.0188 Loss_G: 1.1025\n",
      "[0/15][144/300] Loss_D: 1.3944 Loss_G: 9.0039\n",
      "[0/15][145/300] Loss_D: 1.0853 Loss_G: 5.1892\n",
      "[0/15][146/300] Loss_D: 0.2001 Loss_G: 3.1238\n",
      "[0/15][147/300] Loss_D: 1.1013 Loss_G: 7.7946\n",
      "[0/15][148/300] Loss_D: 0.8117 Loss_G: 4.2300\n",
      "[0/15][149/300] Loss_D: 0.2532 Loss_G: 3.0394\n",
      "[0/15][150/300] Loss_D: 0.2728 Loss_G: 5.6813\n",
      "[0/15][151/300] Loss_D: 0.1852 Loss_G: 5.4405\n",
      "[0/15][152/300] Loss_D: 0.0739 Loss_G: 4.7206\n",
      "[0/15][153/300] Loss_D: 0.0927 Loss_G: 4.2431\n",
      "[0/15][154/300] Loss_D: 0.1014 Loss_G: 4.6996\n",
      "[0/15][155/300] Loss_D: 0.0901 Loss_G: 4.6853\n",
      "[0/15][156/300] Loss_D: 0.1100 Loss_G: 4.3811\n",
      "[0/15][157/300] Loss_D: 0.1393 Loss_G: 4.4890\n",
      "[0/15][158/300] Loss_D: 0.1644 Loss_G: 4.0190\n",
      "[0/15][159/300] Loss_D: 0.1960 Loss_G: 4.3000\n",
      "[0/15][160/300] Loss_D: 0.1557 Loss_G: 4.3303\n",
      "[0/15][161/300] Loss_D: 0.1837 Loss_G: 4.0844\n",
      "[0/15][162/300] Loss_D: 0.2025 Loss_G: 4.6717\n",
      "[0/15][163/300] Loss_D: 0.1851 Loss_G: 4.3450\n",
      "[0/15][164/300] Loss_D: 0.2016 Loss_G: 4.5281\n",
      "[0/15][165/300] Loss_D: 0.3072 Loss_G: 4.3686\n",
      "[0/15][166/300] Loss_D: 0.2430 Loss_G: 4.2296\n",
      "[0/15][167/300] Loss_D: 0.1800 Loss_G: 4.3930\n",
      "[0/15][168/300] Loss_D: 0.1354 Loss_G: 4.6440\n",
      "[0/15][169/300] Loss_D: 0.1248 Loss_G: 4.7550\n",
      "[0/15][170/300] Loss_D: 0.1316 Loss_G: 4.8776\n",
      "[0/15][171/300] Loss_D: 0.1274 Loss_G: 4.3515\n",
      "[0/15][172/300] Loss_D: 0.1503 Loss_G: 4.5250\n",
      "[0/15][173/300] Loss_D: 0.1645 Loss_G: 3.9887\n",
      "[0/15][174/300] Loss_D: 0.2200 Loss_G: 5.0525\n",
      "[0/15][175/300] Loss_D: 0.2751 Loss_G: 2.3865\n",
      "[0/15][176/300] Loss_D: 0.4643 Loss_G: 7.9749\n",
      "[0/15][177/300] Loss_D: 0.4108 Loss_G: 4.6352\n",
      "[0/15][178/300] Loss_D: 0.1686 Loss_G: 3.5852\n",
      "[0/15][179/300] Loss_D: 0.1819 Loss_G: 5.3464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/15][180/300] Loss_D: 0.1543 Loss_G: 4.6999\n",
      "[0/15][181/300] Loss_D: 0.1762 Loss_G: 3.8307\n",
      "[0/15][182/300] Loss_D: 0.2139 Loss_G: 5.3802\n",
      "[0/15][183/300] Loss_D: 0.1520 Loss_G: 4.6195\n",
      "[0/15][184/300] Loss_D: 0.1110 Loss_G: 4.4760\n",
      "[0/15][185/300] Loss_D: 0.1253 Loss_G: 5.0793\n",
      "[0/15][186/300] Loss_D: 0.1186 Loss_G: 4.8008\n",
      "[0/15][187/300] Loss_D: 0.1290 Loss_G: 4.2151\n",
      "[0/15][188/300] Loss_D: 0.1457 Loss_G: 4.6448\n",
      "[0/15][189/300] Loss_D: 0.1416 Loss_G: 4.1350\n",
      "[0/15][190/300] Loss_D: 0.1452 Loss_G: 4.0230\n",
      "[0/15][191/300] Loss_D: 0.1561 Loss_G: 4.4472\n",
      "[0/15][192/300] Loss_D: 0.1124 Loss_G: 4.7636\n",
      "[0/15][193/300] Loss_D: 0.1149 Loss_G: 3.8163\n",
      "[0/15][194/300] Loss_D: 0.1176 Loss_G: 4.7235\n",
      "[0/15][195/300] Loss_D: 0.1186 Loss_G: 4.0528\n",
      "[0/15][196/300] Loss_D: 0.1487 Loss_G: 4.1813\n",
      "[0/15][197/300] Loss_D: 0.1236 Loss_G: 4.6412\n",
      "[0/15][198/300] Loss_D: 0.1160 Loss_G: 3.7319\n",
      "[0/15][199/300] Loss_D: 0.1358 Loss_G: 5.1982\n",
      "[0/15][200/300] Loss_D: 0.1496 Loss_G: 3.0600\n",
      "[0/15][201/300] Loss_D: 0.1535 Loss_G: 5.7575\n",
      "[0/15][202/300] Loss_D: 0.1221 Loss_G: 3.9127\n",
      "[0/15][203/300] Loss_D: 0.0674 Loss_G: 4.2184\n",
      "[0/15][204/300] Loss_D: 0.0780 Loss_G: 4.5725\n",
      "[0/15][205/300] Loss_D: 0.0546 Loss_G: 4.8353\n",
      "[0/15][206/300] Loss_D: 0.0689 Loss_G: 4.1800\n",
      "[0/15][207/300] Loss_D: 0.0824 Loss_G: 4.5110\n",
      "[0/15][208/300] Loss_D: 0.0745 Loss_G: 4.0933\n",
      "[0/15][209/300] Loss_D: 0.0804 Loss_G: 4.6098\n",
      "[0/15][210/300] Loss_D: 0.1242 Loss_G: 3.6907\n",
      "[0/15][211/300] Loss_D: 0.2242 Loss_G: 6.3363\n",
      "[0/15][212/300] Loss_D: 0.4074 Loss_G: 0.2406\n",
      "[0/15][213/300] Loss_D: 2.5942 Loss_G: 18.6889\n",
      "[0/15][214/300] Loss_D: 9.6184 Loss_G: 7.8041\n",
      "[0/15][215/300] Loss_D: 0.3938 Loss_G: 1.4719\n",
      "[0/15][216/300] Loss_D: 2.8074 Loss_G: 14.0514\n",
      "[0/15][217/300] Loss_D: 9.6901 Loss_G: 3.7979\n",
      "[0/15][218/300] Loss_D: 0.4021 Loss_G: 2.9178\n",
      "[0/15][219/300] Loss_D: 0.5488 Loss_G: 3.8019\n",
      "[0/15][220/300] Loss_D: 0.4142 Loss_G: 2.6092\n",
      "[0/15][221/300] Loss_D: 0.5846 Loss_G: 5.4149\n",
      "[0/15][222/300] Loss_D: 0.6601 Loss_G: 1.4091\n",
      "[0/15][223/300] Loss_D: 1.3974 Loss_G: 8.1875\n",
      "[0/15][224/300] Loss_D: 2.5575 Loss_G: 1.4613\n",
      "[0/15][225/300] Loss_D: 1.7404 Loss_G: 5.3381\n",
      "[0/15][226/300] Loss_D: 0.5068 Loss_G: 4.7510\n",
      "[0/15][227/300] Loss_D: 0.5852 Loss_G: 3.2371\n",
      "[0/15][228/300] Loss_D: 0.3988 Loss_G: 3.6843\n",
      "[0/15][229/300] Loss_D: 0.2580 Loss_G: 3.9757\n",
      "[0/15][230/300] Loss_D: 0.1653 Loss_G: 4.4225\n",
      "[0/15][231/300] Loss_D: 0.1640 Loss_G: 4.3095\n",
      "[0/15][232/300] Loss_D: 0.2246 Loss_G: 4.4310\n",
      "[0/15][233/300] Loss_D: 0.2985 Loss_G: 4.1910\n",
      "[0/15][234/300] Loss_D: 0.4433 Loss_G: 3.5837\n",
      "[0/15][235/300] Loss_D: 0.4451 Loss_G: 2.5759\n",
      "[0/15][236/300] Loss_D: 0.4882 Loss_G: 3.7031\n",
      "[0/15][237/300] Loss_D: 0.3534 Loss_G: 2.9686\n",
      "[0/15][238/300] Loss_D: 0.2660 Loss_G: 3.2203\n",
      "[0/15][239/300] Loss_D: 0.2428 Loss_G: 3.8630\n",
      "[0/15][240/300] Loss_D: 0.2637 Loss_G: 3.2446\n",
      "[0/15][241/300] Loss_D: 0.2516 Loss_G: 3.2638\n",
      "[0/15][242/300] Loss_D: 0.2151 Loss_G: 3.6621\n",
      "[0/15][243/300] Loss_D: 0.2310 Loss_G: 3.2607\n",
      "[0/15][244/300] Loss_D: 0.2458 Loss_G: 3.6365\n",
      "[0/15][245/300] Loss_D: 0.1745 Loss_G: 3.7783\n",
      "[0/15][246/300] Loss_D: 0.2039 Loss_G: 3.2996\n",
      "[0/15][247/300] Loss_D: 0.2104 Loss_G: 3.6787\n",
      "[0/15][248/300] Loss_D: 0.2359 Loss_G: 3.4031\n",
      "[0/15][249/300] Loss_D: 0.2244 Loss_G: 3.2106\n",
      "[0/15][250/300] Loss_D: 0.2288 Loss_G: 3.4256\n",
      "[0/15][251/300] Loss_D: 0.2322 Loss_G: 3.9118\n",
      "[0/15][252/300] Loss_D: 0.2455 Loss_G: 3.2524\n",
      "[0/15][253/300] Loss_D: 0.2062 Loss_G: 3.6078\n",
      "[0/15][254/300] Loss_D: 0.1965 Loss_G: 3.8789\n",
      "[0/15][255/300] Loss_D: 0.1527 Loss_G: 3.7750\n",
      "[0/15][256/300] Loss_D: 0.1309 Loss_G: 3.8683\n",
      "[0/15][257/300] Loss_D: 0.1118 Loss_G: 4.3018\n",
      "[0/15][258/300] Loss_D: 0.1215 Loss_G: 4.1632\n",
      "[0/15][259/300] Loss_D: 0.1339 Loss_G: 3.6535\n",
      "[0/15][260/300] Loss_D: 0.1629 Loss_G: 3.6782\n",
      "[0/15][261/300] Loss_D: 0.1714 Loss_G: 3.8881\n",
      "[0/15][262/300] Loss_D: 0.2100 Loss_G: 3.3263\n",
      "[0/15][263/300] Loss_D: 0.2525 Loss_G: 3.9143\n",
      "[0/15][264/300] Loss_D: 0.2911 Loss_G: 2.2599\n",
      "[0/15][265/300] Loss_D: 0.3648 Loss_G: 5.1842\n",
      "[0/15][266/300] Loss_D: 0.4983 Loss_G: 1.1712\n",
      "[0/15][267/300] Loss_D: 0.7744 Loss_G: 8.6736\n",
      "[0/15][268/300] Loss_D: 1.0900 Loss_G: 2.4322\n",
      "[0/15][269/300] Loss_D: 0.5653 Loss_G: 4.7149\n",
      "[0/15][270/300] Loss_D: 0.1347 Loss_G: 5.6100\n",
      "[0/15][271/300] Loss_D: 0.3648 Loss_G: 2.2200\n",
      "[0/15][272/300] Loss_D: 0.5119 Loss_G: 5.5794\n",
      "[0/15][273/300] Loss_D: 0.7442 Loss_G: 1.1553\n",
      "[0/15][274/300] Loss_D: 0.7218 Loss_G: 7.1242\n",
      "[0/15][275/300] Loss_D: 0.3461 Loss_G: 5.7729\n",
      "[0/15][276/300] Loss_D: 0.1004 Loss_G: 3.5543\n",
      "[0/15][277/300] Loss_D: 0.1829 Loss_G: 3.6885\n",
      "[0/15][278/300] Loss_D: 0.1653 Loss_G: 4.7865\n",
      "[0/15][279/300] Loss_D: 0.2166 Loss_G: 3.4541\n",
      "[0/15][280/300] Loss_D: 0.1736 Loss_G: 3.3514\n",
      "[0/15][281/300] Loss_D: 0.1493 Loss_G: 4.1883\n",
      "[0/15][282/300] Loss_D: 0.1640 Loss_G: 3.5240\n",
      "[0/15][283/300] Loss_D: 0.1739 Loss_G: 4.2102\n",
      "[0/15][284/300] Loss_D: 0.1469 Loss_G: 3.7822\n",
      "[0/15][285/300] Loss_D: 0.0990 Loss_G: 3.8827\n",
      "[0/15][286/300] Loss_D: 0.1268 Loss_G: 3.6703\n",
      "[0/15][287/300] Loss_D: 0.0962 Loss_G: 4.3482\n",
      "[0/15][288/300] Loss_D: 0.1062 Loss_G: 4.0965\n",
      "[0/15][289/300] Loss_D: 0.0820 Loss_G: 3.9088\n",
      "[0/15][290/300] Loss_D: 0.0657 Loss_G: 4.0894\n",
      "[0/15][291/300] Loss_D: 0.0854 Loss_G: 4.0369\n",
      "[0/15][292/300] Loss_D: 0.0855 Loss_G: 4.0719\n",
      "[0/15][293/300] Loss_D: 0.1188 Loss_G: 3.6446\n",
      "[0/15][294/300] Loss_D: 0.0944 Loss_G: 4.0682\n",
      "[0/15][295/300] Loss_D: 0.1211 Loss_G: 3.7744\n",
      "[0/15][296/300] Loss_D: 0.1530 Loss_G: 3.5852\n",
      "[0/15][297/300] Loss_D: 0.2184 Loss_G: 2.9752\n",
      "[0/15][298/300] Loss_D: 0.1883 Loss_G: 4.5848\n",
      "[0/15][299/300] Loss_D: 0.1874 Loss_G: 2.4274\n",
      "[1/15][0/300] Loss_D: 0.2470 Loss_G: 6.0438\n",
      "[1/15][1/300] Loss_D: 0.4022 Loss_G: 0.9803\n",
      "[1/15][2/300] Loss_D: 0.8860 Loss_G: 11.3061\n",
      "[1/15][3/300] Loss_D: 2.9480 Loss_G: 3.2383\n",
      "[1/15][4/300] Loss_D: 0.2431 Loss_G: 3.0540\n",
      "[1/15][5/300] Loss_D: 0.4869 Loss_G: 1.7879\n",
      "[1/15][6/300] Loss_D: 0.8625 Loss_G: 9.4046\n",
      "[1/15][7/300] Loss_D: 6.3761 Loss_G: 0.0018\n",
      "[1/15][8/300] Loss_D: 7.0430 Loss_G: 4.5539\n",
      "[1/15][9/300] Loss_D: 1.0387 Loss_G: 3.5772\n",
      "[1/15][10/300] Loss_D: 1.0437 Loss_G: 0.7651\n",
      "[1/15][11/300] Loss_D: 1.5511 Loss_G: 6.5715\n",
      "[1/15][12/300] Loss_D: 2.0329 Loss_G: 2.3559\n",
      "[1/15][13/300] Loss_D: 0.8766 Loss_G: 2.8953\n",
      "[1/15][14/300] Loss_D: 0.7907 Loss_G: 5.0364\n",
      "[1/15][15/300] Loss_D: 1.2221 Loss_G: 1.2860\n",
      "[1/15][16/300] Loss_D: 0.6563 Loss_G: 4.2181\n",
      "[1/15][17/300] Loss_D: 0.2929 Loss_G: 4.1820\n",
      "[1/15][18/300] Loss_D: 0.2874 Loss_G: 2.9497\n",
      "[1/15][19/300] Loss_D: 0.6562 Loss_G: 4.5358\n",
      "[1/15][20/300] Loss_D: 0.8875 Loss_G: 2.9883\n",
      "[1/15][21/300] Loss_D: 1.4341 Loss_G: 2.9541\n",
      "[1/15][22/300] Loss_D: 1.0050 Loss_G: 1.6014\n",
      "[1/15][23/300] Loss_D: 0.3920 Loss_G: 4.9337\n",
      "[1/15][24/300] Loss_D: 0.1200 Loss_G: 5.3861\n",
      "[1/15][25/300] Loss_D: 0.1784 Loss_G: 3.9366\n",
      "[1/15][26/300] Loss_D: 0.2608 Loss_G: 3.6404\n",
      "[1/15][27/300] Loss_D: 0.3289 Loss_G: 3.8464\n",
      "[1/15][28/300] Loss_D: 0.3551 Loss_G: 3.5697\n",
      "[1/15][29/300] Loss_D: 0.3917 Loss_G: 2.9456\n",
      "[1/15][30/300] Loss_D: 0.5153 Loss_G: 3.4777\n",
      "[1/15][31/300] Loss_D: 0.5065 Loss_G: 2.3449\n",
      "[1/15][32/300] Loss_D: 0.5096 Loss_G: 3.8565\n",
      "[1/15][33/300] Loss_D: 0.3796 Loss_G: 3.2115\n",
      "[1/15][34/300] Loss_D: 0.4009 Loss_G: 2.3725\n",
      "[1/15][35/300] Loss_D: 0.3663 Loss_G: 4.1930\n",
      "[1/15][36/300] Loss_D: 0.2427 Loss_G: 3.8697\n",
      "[1/15][37/300] Loss_D: 0.1995 Loss_G: 3.0643\n",
      "[1/15][38/300] Loss_D: 0.1613 Loss_G: 3.5801\n",
      "[1/15][39/300] Loss_D: 0.1547 Loss_G: 4.0658\n",
      "[1/15][40/300] Loss_D: 0.1705 Loss_G: 3.7274\n",
      "[1/15][41/300] Loss_D: 0.2895 Loss_G: 3.2411\n",
      "[1/15][42/300] Loss_D: 0.3154 Loss_G: 3.1623\n",
      "[1/15][43/300] Loss_D: 0.4185 Loss_G: 2.3960\n",
      "[1/15][44/300] Loss_D: 0.3675 Loss_G: 3.8285\n",
      "[1/15][45/300] Loss_D: 0.3319 Loss_G: 2.2586\n",
      "[1/15][46/300] Loss_D: 0.2677 Loss_G: 3.7893\n",
      "[1/15][47/300] Loss_D: 0.1420 Loss_G: 4.1272\n",
      "[1/15][48/300] Loss_D: 0.1690 Loss_G: 3.0652\n",
      "[1/15][49/300] Loss_D: 0.1389 Loss_G: 3.3094\n",
      "[1/15][50/300] Loss_D: 0.2099 Loss_G: 4.1177\n",
      "[1/15][51/300] Loss_D: 0.2461 Loss_G: 2.5733\n",
      "[1/15][52/300] Loss_D: 0.2496 Loss_G: 3.3135\n",
      "[1/15][53/300] Loss_D: 0.2139 Loss_G: 2.9359\n",
      "[1/15][54/300] Loss_D: 0.2183 Loss_G: 3.2911\n",
      "[1/15][55/300] Loss_D: 0.2126 Loss_G: 2.8204\n",
      "[1/15][56/300] Loss_D: 0.2139 Loss_G: 3.2748\n",
      "[1/15][57/300] Loss_D: 0.2031 Loss_G: 3.1095\n",
      "[1/15][58/300] Loss_D: 0.1890 Loss_G: 2.9458\n",
      "[1/15][59/300] Loss_D: 0.1676 Loss_G: 3.5475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15][60/300] Loss_D: 0.1779 Loss_G: 3.3604\n",
      "[1/15][61/300] Loss_D: 0.1611 Loss_G: 3.3895\n",
      "[1/15][62/300] Loss_D: 0.1630 Loss_G: 3.3689\n",
      "[1/15][63/300] Loss_D: 0.1944 Loss_G: 3.0481\n",
      "[1/15][64/300] Loss_D: 0.1676 Loss_G: 3.4232\n",
      "[1/15][65/300] Loss_D: 0.1582 Loss_G: 3.2909\n",
      "[1/15][66/300] Loss_D: 0.1916 Loss_G: 3.5329\n",
      "[1/15][67/300] Loss_D: 0.1668 Loss_G: 3.1787\n",
      "[1/15][68/300] Loss_D: 0.1560 Loss_G: 3.1306\n",
      "[1/15][69/300] Loss_D: 0.1724 Loss_G: 3.3666\n",
      "[1/15][70/300] Loss_D: 0.2061 Loss_G: 3.1097\n",
      "[1/15][71/300] Loss_D: 0.1556 Loss_G: 3.4315\n",
      "[1/15][72/300] Loss_D: 0.1671 Loss_G: 2.9980\n",
      "[1/15][73/300] Loss_D: 0.1916 Loss_G: 3.4929\n",
      "[1/15][74/300] Loss_D: 0.1663 Loss_G: 3.2184\n",
      "[1/15][75/300] Loss_D: 0.1708 Loss_G: 3.2131\n",
      "[1/15][76/300] Loss_D: 0.1425 Loss_G: 3.7132\n",
      "[1/15][77/300] Loss_D: 0.1621 Loss_G: 3.0219\n",
      "[1/15][78/300] Loss_D: 0.1670 Loss_G: 3.3326\n",
      "[1/15][79/300] Loss_D: 0.1452 Loss_G: 3.6048\n",
      "[1/15][80/300] Loss_D: 0.1460 Loss_G: 3.0249\n",
      "[1/15][81/300] Loss_D: 0.1493 Loss_G: 3.6838\n",
      "[1/15][82/300] Loss_D: 0.1489 Loss_G: 2.9585\n",
      "[1/15][83/300] Loss_D: 0.1301 Loss_G: 3.8683\n",
      "[1/15][84/300] Loss_D: 0.1418 Loss_G: 3.3818\n",
      "[1/15][85/300] Loss_D: 0.1091 Loss_G: 3.2924\n",
      "[1/15][86/300] Loss_D: 0.1152 Loss_G: 3.4250\n",
      "[1/15][87/300] Loss_D: 0.1420 Loss_G: 3.6885\n",
      "[1/15][88/300] Loss_D: 0.1482 Loss_G: 3.2597\n",
      "[1/15][89/300] Loss_D: 0.1372 Loss_G: 3.0392\n",
      "[1/15][90/300] Loss_D: 0.1394 Loss_G: 3.9597\n",
      "[1/15][91/300] Loss_D: 0.1853 Loss_G: 2.0538\n",
      "[1/15][92/300] Loss_D: 0.2298 Loss_G: 5.7229\n",
      "[1/15][93/300] Loss_D: 0.5594 Loss_G: 0.1258\n",
      "[1/15][94/300] Loss_D: 2.8660 Loss_G: 13.6337\n",
      "[1/15][95/300] Loss_D: 7.6979 Loss_G: 4.4144\n",
      "[1/15][96/300] Loss_D: 0.3941 Loss_G: 0.1695\n",
      "[1/15][97/300] Loss_D: 2.7973 Loss_G: 8.3679\n",
      "[1/15][98/300] Loss_D: 3.8243 Loss_G: 1.5643\n",
      "[1/15][99/300] Loss_D: 0.5433 Loss_G: 4.5189\n",
      "[1/15][100/300] Loss_D: 0.7694 Loss_G: 1.2865\n",
      "[1/15][101/300] Loss_D: 0.7903 Loss_G: 3.8290\n",
      "[1/15][102/300] Loss_D: 0.6935 Loss_G: 1.2767\n",
      "[1/15][103/300] Loss_D: 0.7375 Loss_G: 3.9844\n",
      "[1/15][104/300] Loss_D: 0.9279 Loss_G: 0.7900\n",
      "[1/15][105/300] Loss_D: 1.1018 Loss_G: 4.2698\n",
      "[1/15][106/300] Loss_D: 1.0714 Loss_G: 1.3418\n",
      "[1/15][107/300] Loss_D: 0.5982 Loss_G: 2.5789\n",
      "[1/15][108/300] Loss_D: 0.4354 Loss_G: 3.2412\n",
      "[1/15][109/300] Loss_D: 0.4707 Loss_G: 2.0584\n",
      "[1/15][110/300] Loss_D: 0.4537 Loss_G: 2.9035\n",
      "[1/15][111/300] Loss_D: 0.4025 Loss_G: 2.6884\n",
      "[1/15][112/300] Loss_D: 0.3391 Loss_G: 2.9494\n",
      "[1/15][113/300] Loss_D: 0.3762 Loss_G: 2.8222\n",
      "[1/15][114/300] Loss_D: 0.3137 Loss_G: 3.0760\n",
      "[1/15][115/300] Loss_D: 0.3255 Loss_G: 3.1650\n",
      "[1/15][116/300] Loss_D: 0.4136 Loss_G: 2.4494\n",
      "[1/15][117/300] Loss_D: 0.3963 Loss_G: 2.6259\n",
      "[1/15][118/300] Loss_D: 0.3542 Loss_G: 3.5023\n",
      "[1/15][119/300] Loss_D: 0.3976 Loss_G: 2.3255\n",
      "[1/15][120/300] Loss_D: 0.3875 Loss_G: 3.2258\n",
      "[1/15][121/300] Loss_D: 0.3685 Loss_G: 2.8260\n",
      "[1/15][122/300] Loss_D: 0.3210 Loss_G: 3.0197\n",
      "[1/15][123/300] Loss_D: 0.3249 Loss_G: 3.1063\n",
      "[1/15][124/300] Loss_D: 0.3083 Loss_G: 3.1096\n",
      "[1/15][125/300] Loss_D: 0.2329 Loss_G: 3.4669\n",
      "[1/15][126/300] Loss_D: 0.3070 Loss_G: 2.9414\n",
      "[1/15][127/300] Loss_D: 0.2835 Loss_G: 3.3033\n",
      "[1/15][128/300] Loss_D: 0.2830 Loss_G: 3.2361\n",
      "[1/15][129/300] Loss_D: 0.3130 Loss_G: 2.8648\n",
      "[1/15][130/300] Loss_D: 0.3357 Loss_G: 3.3794\n",
      "[1/15][131/300] Loss_D: 0.4198 Loss_G: 2.0999\n",
      "[1/15][132/300] Loss_D: 0.2827 Loss_G: 3.7731\n",
      "[1/15][133/300] Loss_D: 0.1960 Loss_G: 3.4318\n",
      "[1/15][134/300] Loss_D: 0.2324 Loss_G: 2.7874\n",
      "[1/15][135/300] Loss_D: 0.2309 Loss_G: 4.0691\n",
      "[1/15][136/300] Loss_D: 0.3299 Loss_G: 2.0663\n",
      "[1/15][137/300] Loss_D: 0.3917 Loss_G: 4.4804\n",
      "[1/15][138/300] Loss_D: 0.3717 Loss_G: 2.4001\n",
      "[1/15][139/300] Loss_D: 0.2726 Loss_G: 3.5196\n",
      "[1/15][140/300] Loss_D: 0.2523 Loss_G: 3.0241\n",
      "[1/15][141/300] Loss_D: 0.2260 Loss_G: 3.4788\n",
      "[1/15][142/300] Loss_D: 0.2429 Loss_G: 2.6482\n",
      "[1/15][143/300] Loss_D: 0.2522 Loss_G: 3.4443\n",
      "[1/15][144/300] Loss_D: 0.2070 Loss_G: 3.2570\n",
      "[1/15][145/300] Loss_D: 0.2102 Loss_G: 2.6124\n",
      "[1/15][146/300] Loss_D: 0.2474 Loss_G: 3.7948\n",
      "[1/15][147/300] Loss_D: 0.2015 Loss_G: 2.9454\n",
      "[1/15][148/300] Loss_D: 0.2148 Loss_G: 2.9329\n",
      "[1/15][149/300] Loss_D: 0.2461 Loss_G: 3.3189\n",
      "[1/15][150/300] Loss_D: 0.2330 Loss_G: 2.5815\n",
      "[1/15][151/300] Loss_D: 0.2466 Loss_G: 3.5036\n",
      "[1/15][152/300] Loss_D: 0.2062 Loss_G: 2.9847\n",
      "[1/15][153/300] Loss_D: 0.2178 Loss_G: 3.0285\n",
      "[1/15][154/300] Loss_D: 0.1978 Loss_G: 2.8196\n",
      "[1/15][155/300] Loss_D: 0.2511 Loss_G: 3.9844\n",
      "[1/15][156/300] Loss_D: 0.3360 Loss_G: 1.1378\n",
      "[1/15][157/300] Loss_D: 0.7101 Loss_G: 7.9230\n",
      "[1/15][158/300] Loss_D: 1.9191 Loss_G: 0.1486\n",
      "[1/15][159/300] Loss_D: 2.6804 Loss_G: 8.5531\n",
      "[1/15][160/300] Loss_D: 3.0201 Loss_G: 0.9530\n",
      "[1/15][161/300] Loss_D: 1.1402 Loss_G: 3.9254\n",
      "[1/15][162/300] Loss_D: 0.5364 Loss_G: 2.7581\n",
      "[1/15][163/300] Loss_D: 0.4416 Loss_G: 2.4195\n",
      "[1/15][164/300] Loss_D: 0.4590 Loss_G: 3.6302\n",
      "[1/15][165/300] Loss_D: 0.4945 Loss_G: 1.9104\n",
      "[1/15][166/300] Loss_D: 0.5156 Loss_G: 3.7250\n",
      "[1/15][167/300] Loss_D: 0.3310 Loss_G: 2.7782\n",
      "[1/15][168/300] Loss_D: 0.3152 Loss_G: 2.3893\n",
      "[1/15][169/300] Loss_D: 0.3060 Loss_G: 3.3630\n",
      "[1/15][170/300] Loss_D: 0.3837 Loss_G: 2.0361\n",
      "[1/15][171/300] Loss_D: 0.3317 Loss_G: 2.8708\n",
      "[1/15][172/300] Loss_D: 0.2084 Loss_G: 3.1937\n",
      "[1/15][173/300] Loss_D: 0.2320 Loss_G: 2.9052\n",
      "[1/15][174/300] Loss_D: 0.2744 Loss_G: 2.8137\n",
      "[1/15][175/300] Loss_D: 0.2142 Loss_G: 3.4371\n",
      "[1/15][176/300] Loss_D: 0.3457 Loss_G: 1.6576\n",
      "[1/15][177/300] Loss_D: 0.4061 Loss_G: 4.3389\n",
      "[1/15][178/300] Loss_D: 0.2764 Loss_G: 3.2281\n",
      "[1/15][179/300] Loss_D: 0.2168 Loss_G: 2.3048\n",
      "[1/15][180/300] Loss_D: 0.3166 Loss_G: 4.2260\n",
      "[1/15][181/300] Loss_D: 0.3423 Loss_G: 2.3021\n",
      "[1/15][182/300] Loss_D: 0.3615 Loss_G: 3.4945\n",
      "[1/15][183/300] Loss_D: 0.2447 Loss_G: 2.7797\n",
      "[1/15][184/300] Loss_D: 0.2051 Loss_G: 2.9252\n",
      "[1/15][185/300] Loss_D: 0.2081 Loss_G: 2.9400\n",
      "[1/15][186/300] Loss_D: 0.2246 Loss_G: 2.7595\n",
      "[1/15][187/300] Loss_D: 0.1846 Loss_G: 3.2227\n",
      "[1/15][188/300] Loss_D: 0.2191 Loss_G: 2.8789\n",
      "[1/15][189/300] Loss_D: 0.2402 Loss_G: 2.3916\n",
      "[1/15][190/300] Loss_D: 0.2882 Loss_G: 3.0603\n",
      "[1/15][191/300] Loss_D: 0.1911 Loss_G: 3.3637\n",
      "[1/15][192/300] Loss_D: 0.2599 Loss_G: 2.0903\n",
      "[1/15][193/300] Loss_D: 0.3236 Loss_G: 3.8114\n",
      "[1/15][194/300] Loss_D: 0.3698 Loss_G: 1.4308\n",
      "[1/15][195/300] Loss_D: 0.5664 Loss_G: 6.3710\n",
      "[1/15][196/300] Loss_D: 1.8662 Loss_G: 4.5602\n",
      "[1/15][197/300] Loss_D: 0.5732 Loss_G: 1.1274\n",
      "[1/15][198/300] Loss_D: 1.4755 Loss_G: 11.9743\n",
      "[1/15][199/300] Loss_D: 11.3315 Loss_G: 0.0865\n",
      "[1/15][200/300] Loss_D: 3.1779 Loss_G: 5.5184\n",
      "[1/15][201/300] Loss_D: 0.7330 Loss_G: 4.1283\n",
      "[1/15][202/300] Loss_D: 0.9922 Loss_G: 1.0893\n",
      "[1/15][203/300] Loss_D: 1.7041 Loss_G: 7.5353\n",
      "[1/15][204/300] Loss_D: 4.1655 Loss_G: 1.3322\n",
      "[1/15][205/300] Loss_D: 1.1464 Loss_G: 3.0999\n",
      "[1/15][206/300] Loss_D: 0.7691 Loss_G: 2.6097\n",
      "[1/15][207/300] Loss_D: 0.6310 Loss_G: 2.4359\n",
      "[1/15][208/300] Loss_D: 0.5531 Loss_G: 2.6932\n",
      "[1/15][209/300] Loss_D: 0.4731 Loss_G: 2.8214\n",
      "[1/15][210/300] Loss_D: 0.3715 Loss_G: 2.5896\n",
      "[1/15][211/300] Loss_D: 0.4820 Loss_G: 2.8874\n",
      "[1/15][212/300] Loss_D: 0.3136 Loss_G: 3.0390\n",
      "[1/15][213/300] Loss_D: 0.4052 Loss_G: 1.8619\n",
      "[1/15][214/300] Loss_D: 0.5077 Loss_G: 3.6351\n",
      "[1/15][215/300] Loss_D: 0.4500 Loss_G: 2.1334\n",
      "[1/15][216/300] Loss_D: 0.4648 Loss_G: 2.5619\n",
      "[1/15][217/300] Loss_D: 0.4758 Loss_G: 2.2157\n",
      "[1/15][218/300] Loss_D: 0.4586 Loss_G: 2.6389\n",
      "[1/15][219/300] Loss_D: 0.3591 Loss_G: 2.4923\n",
      "[1/15][220/300] Loss_D: 0.3562 Loss_G: 2.4885\n",
      "[1/15][221/300] Loss_D: 0.3532 Loss_G: 2.2681\n",
      "[1/15][222/300] Loss_D: 0.3658 Loss_G: 2.8674\n",
      "[1/15][223/300] Loss_D: 0.3519 Loss_G: 2.0479\n",
      "[1/15][224/300] Loss_D: 0.3288 Loss_G: 2.7799\n",
      "[1/15][225/300] Loss_D: 0.3061 Loss_G: 2.4630\n",
      "[1/15][226/300] Loss_D: 0.3339 Loss_G: 2.1026\n",
      "[1/15][227/300] Loss_D: 0.3656 Loss_G: 3.2841\n",
      "[1/15][228/300] Loss_D: 0.4896 Loss_G: 1.0322\n",
      "[1/15][229/300] Loss_D: 0.6455 Loss_G: 4.6593\n",
      "[1/15][230/300] Loss_D: 0.6326 Loss_G: 1.6970\n",
      "[1/15][231/300] Loss_D: 0.4140 Loss_G: 2.5163\n",
      "[1/15][232/300] Loss_D: 0.3355 Loss_G: 2.8682\n",
      "[1/15][233/300] Loss_D: 0.3598 Loss_G: 1.9322\n",
      "[1/15][234/300] Loss_D: 0.3505 Loss_G: 2.4627\n",
      "[1/15][235/300] Loss_D: 0.2934 Loss_G: 2.8674\n",
      "[1/15][236/300] Loss_D: 0.2734 Loss_G: 2.6144\n",
      "[1/15][237/300] Loss_D: 0.3082 Loss_G: 2.1387\n",
      "[1/15][238/300] Loss_D: 0.3037 Loss_G: 2.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15][239/300] Loss_D: 0.3309 Loss_G: 2.1009\n",
      "[1/15][240/300] Loss_D: 0.3073 Loss_G: 3.0006\n",
      "[1/15][241/300] Loss_D: 0.3717 Loss_G: 1.9562\n",
      "[1/15][242/300] Loss_D: 0.2976 Loss_G: 3.0123\n",
      "[1/15][243/300] Loss_D: 0.2878 Loss_G: 2.2804\n",
      "[1/15][244/300] Loss_D: 0.2615 Loss_G: 2.8902\n",
      "[1/15][245/300] Loss_D: 0.2496 Loss_G: 2.7834\n",
      "[1/15][246/300] Loss_D: 0.2723 Loss_G: 2.1988\n",
      "[1/15][247/300] Loss_D: 0.3091 Loss_G: 2.7544\n",
      "[1/15][248/300] Loss_D: 0.2645 Loss_G: 2.4173\n",
      "[1/15][249/300] Loss_D: 0.3150 Loss_G: 2.8813\n",
      "[1/15][250/300] Loss_D: 0.3010 Loss_G: 1.8710\n",
      "[1/15][251/300] Loss_D: 0.3139 Loss_G: 3.6269\n",
      "[1/15][252/300] Loss_D: 0.4249 Loss_G: 0.9824\n",
      "[1/15][253/300] Loss_D: 0.7346 Loss_G: 5.9614\n",
      "[1/15][254/300] Loss_D: 1.2676 Loss_G: 0.6227\n",
      "[1/15][255/300] Loss_D: 1.1633 Loss_G: 7.0741\n",
      "[1/15][256/300] Loss_D: 3.3825 Loss_G: 0.1864\n",
      "[1/15][257/300] Loss_D: 2.6628 Loss_G: 3.8116\n",
      "[1/15][258/300] Loss_D: 0.7593 Loss_G: 2.9909\n",
      "[1/15][259/300] Loss_D: 0.6771 Loss_G: 1.1998\n",
      "[1/15][260/300] Loss_D: 0.6362 Loss_G: 3.1176\n",
      "[1/15][261/300] Loss_D: 0.5690 Loss_G: 1.9767\n",
      "[1/15][262/300] Loss_D: 0.4344 Loss_G: 1.9738\n",
      "[1/15][263/300] Loss_D: 0.4353 Loss_G: 3.0430\n",
      "[1/15][264/300] Loss_D: 0.4475 Loss_G: 1.9317\n",
      "[1/15][265/300] Loss_D: 0.3666 Loss_G: 2.3766\n",
      "[1/15][266/300] Loss_D: 0.3195 Loss_G: 2.9047\n",
      "[1/15][267/300] Loss_D: 0.3134 Loss_G: 2.2275\n",
      "[1/15][268/300] Loss_D: 0.2874 Loss_G: 2.1781\n",
      "[1/15][269/300] Loss_D: 0.3543 Loss_G: 2.8796\n",
      "[1/15][270/300] Loss_D: 0.2991 Loss_G: 2.4060\n",
      "[1/15][271/300] Loss_D: 0.2824 Loss_G: 2.5288\n",
      "[1/15][272/300] Loss_D: 0.2628 Loss_G: 2.8134\n",
      "[1/15][273/300] Loss_D: 0.2566 Loss_G: 2.5079\n",
      "[1/15][274/300] Loss_D: 0.2551 Loss_G: 2.6488\n",
      "[1/15][275/300] Loss_D: 0.2808 Loss_G: 2.2981\n",
      "[1/15][276/300] Loss_D: 0.3107 Loss_G: 2.6458\n",
      "[1/15][277/300] Loss_D: 0.2870 Loss_G: 2.3289\n",
      "[1/15][278/300] Loss_D: 0.2924 Loss_G: 2.5308\n",
      "[1/15][279/300] Loss_D: 0.2992 Loss_G: 2.6366\n",
      "[1/15][280/300] Loss_D: 0.2987 Loss_G: 2.1433\n",
      "[1/15][281/300] Loss_D: 0.3233 Loss_G: 3.1168\n",
      "[1/15][282/300] Loss_D: 0.3276 Loss_G: 2.0062\n",
      "[1/15][283/300] Loss_D: 0.2893 Loss_G: 2.8005\n",
      "[1/15][284/300] Loss_D: 0.3045 Loss_G: 2.4273\n",
      "[1/15][285/300] Loss_D: 0.3012 Loss_G: 2.8450\n",
      "[1/15][286/300] Loss_D: 0.2670 Loss_G: 2.2867\n",
      "[1/15][287/300] Loss_D: 0.2597 Loss_G: 2.9448\n",
      "[1/15][288/300] Loss_D: 0.2807 Loss_G: 2.3280\n",
      "[1/15][289/300] Loss_D: 0.2345 Loss_G: 2.5342\n",
      "[1/15][290/300] Loss_D: 0.2616 Loss_G: 2.7770\n",
      "[1/15][291/300] Loss_D: 0.2552 Loss_G: 2.7403\n",
      "[1/15][292/300] Loss_D: 0.2969 Loss_G: 2.1647\n",
      "[1/15][293/300] Loss_D: 0.2411 Loss_G: 3.1890\n",
      "[1/15][294/300] Loss_D: 0.2989 Loss_G: 2.1249\n",
      "[1/15][295/300] Loss_D: 0.2859 Loss_G: 3.0324\n",
      "[1/15][296/300] Loss_D: 0.2173 Loss_G: 2.5026\n",
      "[1/15][297/300] Loss_D: 0.2801 Loss_G: 2.5498\n",
      "[1/15][298/300] Loss_D: 0.2818 Loss_G: 2.7110\n",
      "[1/15][299/300] Loss_D: 0.2469 Loss_G: 2.4055\n",
      "[2/15][0/300] Loss_D: 0.2186 Loss_G: 2.8194\n",
      "[2/15][1/300] Loss_D: 0.2484 Loss_G: 3.1170\n",
      "[2/15][2/300] Loss_D: 0.2704 Loss_G: 2.1309\n",
      "[2/15][3/300] Loss_D: 0.2544 Loss_G: 3.3797\n",
      "[2/15][4/300] Loss_D: 0.2919 Loss_G: 1.7313\n",
      "[2/15][5/300] Loss_D: 0.3195 Loss_G: 3.9460\n",
      "[2/15][6/300] Loss_D: 0.3840 Loss_G: 1.1285\n",
      "[2/15][7/300] Loss_D: 0.6849 Loss_G: 6.6586\n",
      "[2/15][8/300] Loss_D: 1.7544 Loss_G: 2.1355\n",
      "[2/15][9/300] Loss_D: 1.2500 Loss_G: 9.1217\n",
      "[2/15][10/300] Loss_D: 7.9946 Loss_G: 0.0088\n",
      "[2/15][11/300] Loss_D: 4.9012 Loss_G: 5.6608\n",
      "[2/15][12/300] Loss_D: 1.0372 Loss_G: 6.1718\n",
      "[2/15][13/300] Loss_D: 2.5236 Loss_G: 0.1098\n",
      "[2/15][14/300] Loss_D: 3.9153 Loss_G: 3.2285\n",
      "[2/15][15/300] Loss_D: 1.3431 Loss_G: 1.7499\n",
      "[2/15][16/300] Loss_D: 0.7628 Loss_G: 1.2837\n",
      "[2/15][17/300] Loss_D: 0.8287 Loss_G: 2.1982\n",
      "[2/15][18/300] Loss_D: 0.7712 Loss_G: 1.6652\n",
      "[2/15][19/300] Loss_D: 0.6996 Loss_G: 1.7757\n",
      "[2/15][20/300] Loss_D: 0.6211 Loss_G: 2.1269\n",
      "[2/15][21/300] Loss_D: 0.5792 Loss_G: 1.9553\n",
      "[2/15][22/300] Loss_D: 0.5418 Loss_G: 2.2703\n",
      "[2/15][23/300] Loss_D: 0.5780 Loss_G: 1.9941\n",
      "[2/15][24/300] Loss_D: 0.6266 Loss_G: 1.7909\n",
      "[2/15][25/300] Loss_D: 0.6060 Loss_G: 2.2913\n",
      "[2/15][26/300] Loss_D: 0.4848 Loss_G: 1.8792\n",
      "[2/15][27/300] Loss_D: 0.5743 Loss_G: 2.5353\n",
      "[2/15][28/300] Loss_D: 0.6704 Loss_G: 1.2092\n",
      "[2/15][29/300] Loss_D: 0.5674 Loss_G: 2.8703\n",
      "[2/15][30/300] Loss_D: 0.5094 Loss_G: 2.0946\n",
      "[2/15][31/300] Loss_D: 0.5369 Loss_G: 1.7842\n",
      "[2/15][32/300] Loss_D: 0.4886 Loss_G: 2.5353\n",
      "[2/15][33/300] Loss_D: 0.4876 Loss_G: 1.5357\n",
      "[2/15][34/300] Loss_D: 0.4235 Loss_G: 2.8067\n",
      "[2/15][35/300] Loss_D: 0.4347 Loss_G: 1.8724\n",
      "[2/15][36/300] Loss_D: 0.4482 Loss_G: 2.5723\n",
      "[2/15][37/300] Loss_D: 0.4107 Loss_G: 1.9297\n",
      "[2/15][38/300] Loss_D: 0.4101 Loss_G: 2.4742\n",
      "[2/15][39/300] Loss_D: 0.3909 Loss_G: 2.0486\n",
      "[2/15][40/300] Loss_D: 0.4434 Loss_G: 2.3879\n",
      "[2/15][41/300] Loss_D: 0.4164 Loss_G: 1.7146\n",
      "[2/15][42/300] Loss_D: 0.4085 Loss_G: 2.7870\n",
      "[2/15][43/300] Loss_D: 0.3485 Loss_G: 2.0067\n",
      "[2/15][44/300] Loss_D: 0.3794 Loss_G: 2.6091\n",
      "[2/15][45/300] Loss_D: 0.3933 Loss_G: 1.8983\n",
      "[2/15][46/300] Loss_D: 0.3608 Loss_G: 2.7399\n",
      "[2/15][47/300] Loss_D: 0.3258 Loss_G: 2.0560\n",
      "[2/15][48/300] Loss_D: 0.3299 Loss_G: 2.7015\n",
      "[2/15][49/300] Loss_D: 0.3333 Loss_G: 2.0802\n",
      "[2/15][50/300] Loss_D: 0.3624 Loss_G: 2.8192\n",
      "[2/15][51/300] Loss_D: 0.3591 Loss_G: 1.6360\n",
      "[2/15][52/300] Loss_D: 0.4292 Loss_G: 3.3364\n",
      "[2/15][53/300] Loss_D: 0.3891 Loss_G: 1.6300\n",
      "[2/15][54/300] Loss_D: 0.3975 Loss_G: 3.2476\n",
      "[2/15][55/300] Loss_D: 0.4073 Loss_G: 1.5753\n",
      "[2/15][56/300] Loss_D: 0.4621 Loss_G: 3.6199\n",
      "[2/15][57/300] Loss_D: 0.4508 Loss_G: 2.6395\n",
      "[2/15][58/300] Loss_D: 0.3032 Loss_G: 3.5670\n",
      "[2/15][59/300] Loss_D: 0.7324 Loss_G: 0.6551\n",
      "[2/15][60/300] Loss_D: 1.1628 Loss_G: 8.6374\n",
      "[2/15][61/300] Loss_D: 4.9309 Loss_G: 1.1460\n",
      "[2/15][62/300] Loss_D: 0.8370 Loss_G: 1.5063\n",
      "[2/15][63/300] Loss_D: 0.7463 Loss_G: 3.9041\n",
      "[2/15][64/300] Loss_D: 1.2842 Loss_G: 0.4432\n",
      "[2/15][65/300] Loss_D: 1.3601 Loss_G: 3.5409\n",
      "[2/15][66/300] Loss_D: 1.0516 Loss_G: 1.0406\n",
      "[2/15][67/300] Loss_D: 0.8546 Loss_G: 2.5254\n",
      "[2/15][68/300] Loss_D: 0.5936 Loss_G: 1.8298\n",
      "[2/15][69/300] Loss_D: 0.4846 Loss_G: 2.1980\n",
      "[2/15][70/300] Loss_D: 0.4822 Loss_G: 1.7952\n",
      "[2/15][71/300] Loss_D: 0.5370 Loss_G: 2.7715\n",
      "[2/15][72/300] Loss_D: 0.5317 Loss_G: 1.3604\n",
      "[2/15][73/300] Loss_D: 0.5751 Loss_G: 3.2226\n",
      "[2/15][74/300] Loss_D: 0.6464 Loss_G: 1.0102\n",
      "[2/15][75/300] Loss_D: 0.7168 Loss_G: 3.6747\n",
      "[2/15][76/300] Loss_D: 0.6399 Loss_G: 1.3703\n",
      "[2/15][77/300] Loss_D: 0.5133 Loss_G: 2.5038\n",
      "[2/15][78/300] Loss_D: 0.4554 Loss_G: 2.1492\n",
      "[2/15][79/300] Loss_D: 0.3792 Loss_G: 2.2640\n",
      "[2/15][80/300] Loss_D: 0.3705 Loss_G: 2.1879\n",
      "[2/15][81/300] Loss_D: 0.3893 Loss_G: 2.1875\n",
      "[2/15][82/300] Loss_D: 0.3804 Loss_G: 2.6019\n",
      "[2/15][83/300] Loss_D: 0.3942 Loss_G: 1.9100\n",
      "[2/15][84/300] Loss_D: 0.3680 Loss_G: 2.8022\n",
      "[2/15][85/300] Loss_D: 0.3935 Loss_G: 1.5447\n",
      "[2/15][86/300] Loss_D: 0.4694 Loss_G: 3.6133\n",
      "[2/15][87/300] Loss_D: 0.5492 Loss_G: 1.0621\n",
      "[2/15][88/300] Loss_D: 0.4790 Loss_G: 3.2278\n",
      "[2/15][89/300] Loss_D: 0.4092 Loss_G: 1.9880\n",
      "[2/15][90/300] Loss_D: 0.3048 Loss_G: 2.8145\n",
      "[2/15][91/300] Loss_D: 0.3532 Loss_G: 2.0768\n",
      "[2/15][92/300] Loss_D: 0.3117 Loss_G: 2.5823\n",
      "[2/15][93/300] Loss_D: 0.3401 Loss_G: 2.2823\n",
      "[2/15][94/300] Loss_D: 0.2994 Loss_G: 2.5656\n",
      "[2/15][95/300] Loss_D: 0.3288 Loss_G: 2.4204\n",
      "[2/15][96/300] Loss_D: 0.2522 Loss_G: 2.4432\n",
      "[2/15][97/300] Loss_D: 0.2923 Loss_G: 2.6477\n",
      "[2/15][98/300] Loss_D: 0.2618 Loss_G: 2.3634\n",
      "[2/15][99/300] Loss_D: 0.3039 Loss_G: 2.3011\n",
      "[2/15][100/300] Loss_D: 0.3075 Loss_G: 2.8271\n",
      "[2/15][101/300] Loss_D: 0.3258 Loss_G: 2.0826\n",
      "[2/15][102/300] Loss_D: 0.3100 Loss_G: 2.5662\n",
      "[2/15][103/300] Loss_D: 0.3111 Loss_G: 2.3970\n",
      "[2/15][104/300] Loss_D: 0.2712 Loss_G: 3.0126\n",
      "[2/15][105/300] Loss_D: 0.3162 Loss_G: 1.9013\n",
      "[2/15][106/300] Loss_D: 0.3258 Loss_G: 3.6297\n",
      "[2/15][107/300] Loss_D: 0.3550 Loss_G: 2.1779\n",
      "[2/15][108/300] Loss_D: 0.2643 Loss_G: 2.8391\n",
      "[2/15][109/300] Loss_D: 0.2667 Loss_G: 2.3829\n",
      "[2/15][110/300] Loss_D: 0.2747 Loss_G: 2.8952\n",
      "[2/15][111/300] Loss_D: 0.2798 Loss_G: 2.7278\n",
      "[2/15][112/300] Loss_D: 0.2554 Loss_G: 2.3791\n",
      "[2/15][113/300] Loss_D: 0.2401 Loss_G: 3.0617\n",
      "[2/15][114/300] Loss_D: 0.2962 Loss_G: 2.1565\n",
      "[2/15][115/300] Loss_D: 0.2935 Loss_G: 3.4476\n",
      "[2/15][116/300] Loss_D: 0.3330 Loss_G: 2.0661\n",
      "[2/15][117/300] Loss_D: 0.4662 Loss_G: 4.6961\n",
      "[2/15][118/300] Loss_D: 0.9466 Loss_G: 1.6007\n",
      "[2/15][119/300] Loss_D: 0.4785 Loss_G: 3.5485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/15][120/300] Loss_D: 0.8827 Loss_G: 0.2788\n",
      "[2/15][121/300] Loss_D: 2.2950 Loss_G: 6.6307\n",
      "[2/15][122/300] Loss_D: 3.5342 Loss_G: 0.8212\n",
      "[2/15][123/300] Loss_D: 1.0353 Loss_G: 2.2258\n",
      "[2/15][124/300] Loss_D: 0.7662 Loss_G: 2.2064\n",
      "[2/15][125/300] Loss_D: 0.6627 Loss_G: 1.4884\n",
      "[2/15][126/300] Loss_D: 0.7209 Loss_G: 2.6182\n",
      "[2/15][127/300] Loss_D: 0.7539 Loss_G: 1.0786\n",
      "[2/15][128/300] Loss_D: 0.7981 Loss_G: 3.1266\n",
      "[2/15][129/300] Loss_D: 0.7677 Loss_G: 0.9820\n",
      "[2/15][130/300] Loss_D: 0.8909 Loss_G: 3.5774\n",
      "[2/15][131/300] Loss_D: 0.9191 Loss_G: 0.8718\n",
      "[2/15][132/300] Loss_D: 1.1187 Loss_G: 4.0065\n",
      "[2/15][133/300] Loss_D: 1.0749 Loss_G: 0.9877\n",
      "[2/15][134/300] Loss_D: 0.7660 Loss_G: 2.6308\n",
      "[2/15][135/300] Loss_D: 0.5632 Loss_G: 1.9948\n",
      "[2/15][136/300] Loss_D: 0.5667 Loss_G: 1.8908\n",
      "[2/15][137/300] Loss_D: 0.4474 Loss_G: 2.3135\n",
      "[2/15][138/300] Loss_D: 0.4684 Loss_G: 1.9525\n",
      "[2/15][139/300] Loss_D: 0.4422 Loss_G: 2.6483\n",
      "[2/15][140/300] Loss_D: 0.4996 Loss_G: 1.1747\n",
      "[2/15][141/300] Loss_D: 0.7377 Loss_G: 4.7696\n",
      "[2/15][142/300] Loss_D: 1.3841 Loss_G: 0.3186\n",
      "[2/15][143/300] Loss_D: 1.7402 Loss_G: 4.2545\n",
      "[2/15][144/300] Loss_D: 0.9743 Loss_G: 1.5197\n",
      "[2/15][145/300] Loss_D: 0.4959 Loss_G: 1.9611\n",
      "[2/15][146/300] Loss_D: 0.4998 Loss_G: 3.1450\n",
      "[2/15][147/300] Loss_D: 0.5783 Loss_G: 1.5784\n",
      "[2/15][148/300] Loss_D: 0.5114 Loss_G: 2.5864\n",
      "[2/15][149/300] Loss_D: 0.4627 Loss_G: 2.0005\n",
      "[2/15][150/300] Loss_D: 0.4210 Loss_G: 2.2466\n",
      "[2/15][151/300] Loss_D: 0.3848 Loss_G: 2.3687\n",
      "[2/15][152/300] Loss_D: 0.3722 Loss_G: 2.3562\n",
      "[2/15][153/300] Loss_D: 0.4139 Loss_G: 2.0016\n",
      "[2/15][154/300] Loss_D: 0.4343 Loss_G: 2.3542\n",
      "[2/15][155/300] Loss_D: 0.3582 Loss_G: 1.9360\n",
      "[2/15][156/300] Loss_D: 0.4033 Loss_G: 3.2145\n",
      "[2/15][157/300] Loss_D: 0.4105 Loss_G: 1.4318\n",
      "[2/15][158/300] Loss_D: 0.4305 Loss_G: 3.3992\n",
      "[2/15][159/300] Loss_D: 0.3856 Loss_G: 1.6844\n",
      "[2/15][160/300] Loss_D: 0.3972 Loss_G: 3.2553\n",
      "[2/15][161/300] Loss_D: 0.4382 Loss_G: 1.3235\n",
      "[2/15][162/300] Loss_D: 0.5076 Loss_G: 4.1741\n",
      "[2/15][163/300] Loss_D: 0.6608 Loss_G: 0.7584\n",
      "[2/15][164/300] Loss_D: 1.0443 Loss_G: 5.6351\n",
      "[2/15][165/300] Loss_D: 1.4879 Loss_G: 0.6029\n",
      "[2/15][166/300] Loss_D: 1.0303 Loss_G: 4.7691\n",
      "[2/15][167/300] Loss_D: 0.7539 Loss_G: 1.2925\n",
      "[2/15][168/300] Loss_D: 0.4955 Loss_G: 3.6993\n",
      "[2/15][169/300] Loss_D: 0.4715 Loss_G: 2.0197\n",
      "[2/15][170/300] Loss_D: 0.5621 Loss_G: 2.8164\n",
      "[2/15][171/300] Loss_D: 0.4147 Loss_G: 2.0577\n",
      "[2/15][172/300] Loss_D: 0.3946 Loss_G: 2.8216\n",
      "[2/15][173/300] Loss_D: 0.3429 Loss_G: 2.7673\n",
      "[2/15][174/300] Loss_D: 0.2454 Loss_G: 2.6512\n",
      "[2/15][175/300] Loss_D: 0.3329 Loss_G: 2.7810\n",
      "[2/15][176/300] Loss_D: 0.3057 Loss_G: 2.1335\n",
      "[2/15][177/300] Loss_D: 0.3044 Loss_G: 2.9622\n",
      "[2/15][178/300] Loss_D: 0.3272 Loss_G: 2.1146\n",
      "[2/15][179/300] Loss_D: 0.2891 Loss_G: 2.5037\n",
      "[2/15][180/300] Loss_D: 0.3033 Loss_G: 2.6300\n",
      "[2/15][181/300] Loss_D: 0.3150 Loss_G: 2.2850\n",
      "[2/15][182/300] Loss_D: 0.3250 Loss_G: 2.9637\n",
      "[2/15][183/300] Loss_D: 0.3310 Loss_G: 1.7461\n",
      "[2/15][184/300] Loss_D: 0.3712 Loss_G: 3.6581\n",
      "[2/15][185/300] Loss_D: 0.4150 Loss_G: 1.4691\n",
      "[2/15][186/300] Loss_D: 0.3694 Loss_G: 3.3571\n",
      "[2/15][187/300] Loss_D: 0.3432 Loss_G: 2.5171\n",
      "[2/15][188/300] Loss_D: 0.3013 Loss_G: 2.9667\n",
      "[2/15][189/300] Loss_D: 0.2850 Loss_G: 2.2555\n",
      "[2/15][190/300] Loss_D: 0.2679 Loss_G: 3.1208\n",
      "[2/15][191/300] Loss_D: 0.2580 Loss_G: 2.8236\n",
      "[2/15][192/300] Loss_D: 0.2322 Loss_G: 2.7148\n",
      "[2/15][193/300] Loss_D: 0.2690 Loss_G: 2.1074\n",
      "[2/15][194/300] Loss_D: 0.3202 Loss_G: 3.6752\n",
      "[2/15][195/300] Loss_D: 0.2977 Loss_G: 2.0306\n",
      "[2/15][196/300] Loss_D: 0.3191 Loss_G: 2.9620\n",
      "[2/15][197/300] Loss_D: 0.2783 Loss_G: 2.0061\n",
      "[2/15][198/300] Loss_D: 0.3414 Loss_G: 3.7751\n",
      "[2/15][199/300] Loss_D: 0.3500 Loss_G: 1.7645\n",
      "[2/15][200/300] Loss_D: 0.4250 Loss_G: 4.1124\n",
      "[2/15][201/300] Loss_D: 0.5802 Loss_G: 2.0857\n",
      "[2/15][202/300] Loss_D: 0.4345 Loss_G: 3.1058\n",
      "[2/15][203/300] Loss_D: 0.6642 Loss_G: 0.4200\n",
      "[2/15][204/300] Loss_D: 1.5730 Loss_G: 7.7937\n",
      "[2/15][205/300] Loss_D: 3.8740 Loss_G: 0.7331\n",
      "[2/15][206/300] Loss_D: 1.3879 Loss_G: 2.6436\n",
      "[2/15][207/300] Loss_D: 0.7202 Loss_G: 2.3558\n",
      "[2/15][208/300] Loss_D: 0.7714 Loss_G: 1.1874\n",
      "[2/15][209/300] Loss_D: 0.8377 Loss_G: 4.0436\n",
      "[2/15][210/300] Loss_D: 1.1339 Loss_G: 0.5977\n",
      "[2/15][211/300] Loss_D: 1.1488 Loss_G: 3.7066\n",
      "[2/15][212/300] Loss_D: 0.8055 Loss_G: 1.5890\n",
      "[2/15][213/300] Loss_D: 0.5284 Loss_G: 2.7241\n",
      "[2/15][214/300] Loss_D: 0.4212 Loss_G: 2.4811\n",
      "[2/15][215/300] Loss_D: 0.3850 Loss_G: 2.0793\n",
      "[2/15][216/300] Loss_D: 0.3465 Loss_G: 2.7793\n",
      "[2/15][217/300] Loss_D: 0.3650 Loss_G: 2.1532\n",
      "[2/15][218/300] Loss_D: 0.3391 Loss_G: 2.3379\n",
      "[2/15][219/300] Loss_D: 0.3246 Loss_G: 2.9169\n",
      "[2/15][220/300] Loss_D: 0.3552 Loss_G: 1.9304\n",
      "[2/15][221/300] Loss_D: 0.3260 Loss_G: 3.1180\n",
      "[2/15][222/300] Loss_D: 0.3338 Loss_G: 2.0112\n",
      "[2/15][223/300] Loss_D: 0.3745 Loss_G: 2.4587\n",
      "[2/15][224/300] Loss_D: 0.2832 Loss_G: 3.0909\n",
      "[2/15][225/300] Loss_D: 0.3145 Loss_G: 1.7316\n",
      "[2/15][226/300] Loss_D: 0.4223 Loss_G: 4.0920\n",
      "[2/15][227/300] Loss_D: 0.5055 Loss_G: 0.9591\n",
      "[2/15][228/300] Loss_D: 0.7049 Loss_G: 4.5794\n",
      "[2/15][229/300] Loss_D: 0.7043 Loss_G: 1.7924\n",
      "[2/15][230/300] Loss_D: 0.4183 Loss_G: 3.8398\n",
      "[2/15][231/300] Loss_D: 0.8536 Loss_G: 1.4839\n",
      "[2/15][232/300] Loss_D: 0.4921 Loss_G: 3.5065\n",
      "[2/15][233/300] Loss_D: 0.7244 Loss_G: 1.0979\n",
      "[2/15][234/300] Loss_D: 1.0049 Loss_G: 3.8561\n",
      "[2/15][235/300] Loss_D: 1.1679 Loss_G: 0.4202\n",
      "[2/15][236/300] Loss_D: 1.7392 Loss_G: 4.6681\n",
      "[2/15][237/300] Loss_D: 1.8559 Loss_G: 0.8108\n",
      "[2/15][238/300] Loss_D: 1.1137 Loss_G: 3.1714\n",
      "[2/15][239/300] Loss_D: 0.7563 Loss_G: 1.8656\n",
      "[2/15][240/300] Loss_D: 0.5875 Loss_G: 1.8226\n",
      "[2/15][241/300] Loss_D: 0.6068 Loss_G: 2.3014\n",
      "[2/15][242/300] Loss_D: 0.5700 Loss_G: 1.6128\n",
      "[2/15][243/300] Loss_D: 0.6440 Loss_G: 2.5468\n",
      "[2/15][244/300] Loss_D: 0.5055 Loss_G: 1.6929\n",
      "[2/15][245/300] Loss_D: 0.5505 Loss_G: 2.2117\n",
      "[2/15][246/300] Loss_D: 0.5057 Loss_G: 2.2858\n",
      "[2/15][247/300] Loss_D: 0.4549 Loss_G: 1.8911\n",
      "[2/15][248/300] Loss_D: 0.4789 Loss_G: 2.5950\n",
      "[2/15][249/300] Loss_D: 0.4777 Loss_G: 1.4457\n",
      "[2/15][250/300] Loss_D: 0.5212 Loss_G: 3.9215\n",
      "[2/15][251/300] Loss_D: 0.6378 Loss_G: 0.9130\n",
      "[2/15][252/300] Loss_D: 0.7422 Loss_G: 4.1828\n",
      "[2/15][253/300] Loss_D: 0.8111 Loss_G: 1.0340\n",
      "[2/15][254/300] Loss_D: 0.6637 Loss_G: 3.3200\n",
      "[2/15][255/300] Loss_D: 0.4559 Loss_G: 2.1478\n",
      "[2/15][256/300] Loss_D: 0.3716 Loss_G: 2.1586\n",
      "[2/15][257/300] Loss_D: 0.3826 Loss_G: 3.0706\n",
      "[2/15][258/300] Loss_D: 0.4057 Loss_G: 1.7550\n",
      "[2/15][259/300] Loss_D: 0.3792 Loss_G: 3.0563\n",
      "[2/15][260/300] Loss_D: 0.3922 Loss_G: 1.6872\n",
      "[2/15][261/300] Loss_D: 0.3494 Loss_G: 3.6294\n",
      "[2/15][262/300] Loss_D: 0.4187 Loss_G: 1.3985\n",
      "[2/15][263/300] Loss_D: 0.4767 Loss_G: 3.9670\n",
      "[2/15][264/300] Loss_D: 0.4985 Loss_G: 1.4191\n",
      "[2/15][265/300] Loss_D: 0.4583 Loss_G: 3.5971\n",
      "[2/15][266/300] Loss_D: 0.4019 Loss_G: 2.0578\n",
      "[2/15][267/300] Loss_D: 0.3107 Loss_G: 2.4717\n",
      "[2/15][268/300] Loss_D: 0.2762 Loss_G: 3.1328\n",
      "[2/15][269/300] Loss_D: 0.3616 Loss_G: 1.6257\n",
      "[2/15][270/300] Loss_D: 0.4034 Loss_G: 3.0007\n",
      "[2/15][271/300] Loss_D: 0.3096 Loss_G: 2.2890\n",
      "[2/15][272/300] Loss_D: 0.2442 Loss_G: 2.4885\n",
      "[2/15][273/300] Loss_D: 0.3053 Loss_G: 3.1114\n",
      "[2/15][274/300] Loss_D: 0.2542 Loss_G: 2.3305\n",
      "[2/15][275/300] Loss_D: 0.2424 Loss_G: 2.7305\n",
      "[2/15][276/300] Loss_D: 0.2868 Loss_G: 2.6260\n",
      "[2/15][277/300] Loss_D: 0.2900 Loss_G: 1.8100\n",
      "[2/15][278/300] Loss_D: 0.3634 Loss_G: 4.6341\n",
      "[2/15][279/300] Loss_D: 0.7451 Loss_G: 0.3474\n",
      "[2/15][280/300] Loss_D: 1.7079 Loss_G: 7.6135\n",
      "[2/15][281/300] Loss_D: 4.0642 Loss_G: 0.9313\n",
      "[2/15][282/300] Loss_D: 1.2098 Loss_G: 2.0179\n",
      "[2/15][283/300] Loss_D: 0.8522 Loss_G: 3.1311\n",
      "[2/15][284/300] Loss_D: 0.9780 Loss_G: 0.7332\n",
      "[2/15][285/300] Loss_D: 1.1179 Loss_G: 3.4456\n",
      "[2/15][286/300] Loss_D: 0.9984 Loss_G: 1.1114\n",
      "[2/15][287/300] Loss_D: 0.6749 Loss_G: 2.6756\n",
      "[2/15][288/300] Loss_D: 0.5609 Loss_G: 2.3891\n",
      "[2/15][289/300] Loss_D: 0.5147 Loss_G: 1.5367\n",
      "[2/15][290/300] Loss_D: 0.5445 Loss_G: 2.8308\n",
      "[2/15][291/300] Loss_D: 0.4799 Loss_G: 2.0088\n",
      "[2/15][292/300] Loss_D: 0.4535 Loss_G: 2.0473\n",
      "[2/15][293/300] Loss_D: 0.4182 Loss_G: 2.6917\n",
      "[2/15][294/300] Loss_D: 0.3799 Loss_G: 1.9896\n",
      "[2/15][295/300] Loss_D: 0.3866 Loss_G: 2.7510\n",
      "[2/15][296/300] Loss_D: 0.3735 Loss_G: 2.2590\n",
      "[2/15][297/300] Loss_D: 0.3684 Loss_G: 1.9695\n",
      "[2/15][298/300] Loss_D: 0.3678 Loss_G: 3.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/15][299/300] Loss_D: 0.3500 Loss_G: 1.8118\n",
      "[3/15][0/300] Loss_D: 0.3341 Loss_G: 3.2386\n",
      "[3/15][1/300] Loss_D: 0.3221 Loss_G: 2.0802\n",
      "[3/15][2/300] Loss_D: 0.3202 Loss_G: 3.0719\n",
      "[3/15][3/300] Loss_D: 0.3712 Loss_G: 1.3740\n",
      "[3/15][4/300] Loss_D: 0.4664 Loss_G: 3.8808\n",
      "[3/15][5/300] Loss_D: 0.4151 Loss_G: 1.8442\n",
      "[3/15][6/300] Loss_D: 0.3210 Loss_G: 2.6729\n",
      "[3/15][7/300] Loss_D: 0.2714 Loss_G: 3.0229\n",
      "[3/15][8/300] Loss_D: 0.2777 Loss_G: 2.2033\n",
      "[3/15][9/300] Loss_D: 0.2705 Loss_G: 2.9053\n",
      "[3/15][10/300] Loss_D: 0.2484 Loss_G: 2.8934\n"
     ]
    }
   ],
   "source": [
    "#Setting Up the Class Of Generator(This will be a Deconvolutional Network as it converts random noises to images)\n",
    "\n",
    "class G(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.main=nn.Sequential(\n",
    "            nn.ConvTranspose2d(100,512,4,1,0,bias=False),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(64, 1, 4, 2, 1, bias = False),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            \n",
    "        \n",
    "        def forward(self,input):\n",
    "            output=self.main(input)\n",
    "            return output\n",
    "        \n",
    "netG=G()\n",
    "netG.apply(weights_init)\n",
    "\n",
    "            \n",
    "        \n",
    "#Setting Up the Class of Discriminator(This will be a ConvNet)\n",
    "class D(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.main=nn.Sequential( nn.Conv2d(1,64,4,2,1,bias=False),\n",
    "                                    nn.LeakyReLU(0.2, inplace = True),\n",
    "                                    nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.LeakyReLU(0.2, inplace = True),\n",
    "                                    nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.LeakyReLU(0.2, inplace = True),\n",
    "                                    nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.LeakyReLU(0.2, inplace = True),\n",
    "                                    nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "                                    nn.Sigmoid()\n",
    "                                   )\n",
    "            \n",
    "        def forward(self,input):\n",
    "            output=self.main(input)\n",
    "            return output.view(-1)\n",
    "        \n",
    "        \n",
    "netD=D() \n",
    "netD.apply(weights_init)\n",
    "criterion=nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i,data in enumerate(dataloader,0):\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Now we will train the discriminator on real images(mini-batches)\n",
    "        \n",
    "        real,_=data\n",
    "        input=Variable(real)# As pytorch neural network takes tensors as input.\n",
    "        output=netD.forward(input)\n",
    "        target=Variable(torch.ones(input.size()[0]))\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        #Training the Discriminator on Fake Data\n",
    "        noise=Variable(torch.randn(input.size()[0],100,1,1))\n",
    "        fake=netG(noise)\n",
    "        output=netD(fake.detach())\n",
    "        target=Variable(torch.zeros(input.size()[0]))\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        #Improving The Model of Discriminator via BackPropagation\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #Its time to Train the Generator.\n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 15, i, len(dataloader), errD.data[0], errG.data[0]))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)\n",
    "       \n",
    "                 \n",
    "\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
